{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eaced98e-595e-45b3-aa45-99ba59130c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import asyncio\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "import yarl # not necessary?\n",
    "\n",
    "\n",
    "# ANSI colors\n",
    "c = (\n",
    "    \"\\033[0m\",   # End of color\n",
    "    \"\\033[36m\",  # Cyan\n",
    "    \"\\033[91m\",  # Red\n",
    "    \"\\033[35m\",  # Magenta\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1148df5d-bd54-4b6c-b33a-2087072da374",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # TODO: https://medium.com/@sakthiveltvt.thangaraj/introduction-to-nest-asyncio-for-python-developers-afd7bed44768\n",
    "# # TODO: https://www.bing.com/search?pglt=161&q=asynchio+nesting\n",
    "\n",
    "# global results\n",
    "# results = {}\n",
    "# global job_ad_ids\n",
    "# job_ad_ids = []\n",
    "\n",
    "# # Function to scrape one page using Selenium\n",
    "# async def scrape_with_selenium(url):\n",
    "#     options = Options()\n",
    "#     # options.add_argument(\"--headless\")  # Run in headless mode\n",
    "#     driver = webdriver.Chrome(options=options)\n",
    "#     try:\n",
    "#         driver.get(url)\n",
    "#         content = driver.page_source\n",
    "#         print(f\"Scraped {url}\")\n",
    "#         # print(content)\n",
    "#         article = driver.find_elements(By.TAG_NAME, \"article\")\n",
    "#         # print('article: ', article)\n",
    "#         with ThreadPoolExecutor(max_workers=30) as executor:\n",
    "#             tasks = [async_populate_results(element, executor) for element in article]\n",
    "#             results = await asyncio.gather(*tasks)\n",
    "#             # executor.map(populate_results, article)\n",
    "#         return content\n",
    "#     finally:\n",
    "#         driver.quit()\n",
    "\n",
    "# # Async wrapper to run the blocking Selenium code\n",
    "# # async def async_populate_results(element, executor):\n",
    "# #     loop = asyncio.get_running_loop()\n",
    "# #     return await loop.run_in_executor(executor, populate_results, element)\n",
    "\n",
    "# async def populate_results(article):\n",
    "#     print(article)\n",
    "#     print('populating results...')\n",
    "#     # job_ad_id = article.get_attribute(\"data-job-id\")\n",
    "#     # job_ad_ids.append(job_ad_id)\n",
    "#     # results[job_ad_id] = {}\n",
    "#     # elements = article.find_elements(By.CSS_SELECTOR, '[data-automation]')\n",
    "#     # for el in elements:\n",
    "#     #     results[job_ad_id][el.get_attribute(\"data-automation\").replace('-','_')] = el.text\n",
    "#     # # results.append([f'{element.get_attribute(\"data-automation\").replace(\"-\",\"_\")}:{element.text}' for element in elements])\n",
    "\n",
    "# # Async wrapper to run the blocking Selenium code\n",
    "# async def async_scrape(url, executor):\n",
    "#     loop = asyncio.get_running_loop()\n",
    "#     return await loop.run_in_executor(executor, scrape_with_selenium, url)\n",
    "\n",
    "# # Main async function to scrape multiple URLs\n",
    "# async def main(urls):\n",
    "#     results = []\n",
    "#     with ThreadPoolExecutor(max_workers=30) as executor:\n",
    "#         tasks = [async_scrape(url, executor) for url in urls]\n",
    "#         print(len(tasks))\n",
    "#         results = await asyncio.gather(*tasks)\n",
    "#     print(f\"\\nScraped {len(results)} pages.\")\n",
    "\n",
    "# # Example usage\n",
    "# if __name__ == \"__main__\":\n",
    "#     start_time = time.perf_counter()\n",
    "#     urls = [\n",
    "#     f'https://www.seek.com.au/Data-Engineer-jobs/in-Melbourne-VIC-3000?page={x+1}' for x in range(2)\n",
    "#     ]\n",
    "#     await main(urls)\n",
    "#     print('run time = ', time.perf_counter() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f45713-e136-42ad-b0b7-9e63274a4f1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "370cac6c-3ddc-4297-8c61-f3b21306b32e",
   "metadata": {},
   "source": [
    "<h3 style=\"color: #F34222;\">VERSION 2</h3><p><h4 style=\"color: #F3FF22;\">async/await</h4></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86fc61f-8e85-41d6-9f98-7d5e1ef56509",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "52126c0d-9fbe-41bd-94cd-34a3f91fc1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = 1,2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60509080-3cd3-4f62-a44d-c655ba750be5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cfbd1470-dde0-4ca4-a4ed-d868b526de10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped https://www.seek.com.au/Data-Engineer-jobs/in-Melbourne-VIC-3000?page=1\n",
      "Scraped https://www.seek.com.au/Data-Engineer-jobs/in-Melbourne-VIC-3000?page=3\n",
      "Scraped https://www.seek.com.au/Data-Engineer-jobs/in-Melbourne-VIC-3000?page=9\n",
      "Scraped https://www.seek.com.au/Data-Engineer-jobs/in-Melbourne-VIC-3000?page=10\n",
      "Scraped https://www.seek.com.au/Data-Engineer-jobs/in-Melbourne-VIC-3000?page=8\n",
      "Scraped https://www.seek.com.au/Data-Engineer-jobs/in-Melbourne-VIC-3000?page=2\n",
      "Scraped https://www.seek.com.au/Data-Engineer-jobs/in-Melbourne-VIC-3000?page=4\n",
      "Scraped https://www.seek.com.au/Data-Engineer-jobs/in-Melbourne-VIC-3000?page=6\n",
      "Scraped https://www.seek.com.au/Data-Engineer-jobs/in-Melbourne-VIC-3000?page=7\n",
      "Scraped https://www.seek.com.au/Data-Engineer-jobs/in-Melbourne-VIC-3000?page=5\n",
      "articles collected... \u001b[36mcollection time: 1.0660961499997939s | start time: 17128.761386654\u001b[0m\n",
      "stopping page load...\n",
      "articles collected... \u001b[36mcollection time: 1.189876885000558s | start time: 17128.773374657\u001b[0m\n",
      "stopping page load...\n",
      "parse end time =  17131.226100956\n",
      "run time (parsing article) =  1.2631798559996241\n",
      "articles collected... \u001b[36mcollection time: 2.7757960640010424s | start time: 17129.241865769\u001b[0m\n",
      "stopping page load...\n",
      "articles collected... \u001b[36mcollection time: 3.065955497997493s | start time: 17129.166119269\u001b[0m\n",
      "stopping page load...\n",
      "articles collected... \u001b[36mcollection time: 3.572008964001725s | start time: 17129.200607321\u001b[0m\n",
      "stopping page load...\n",
      "articles collected... \u001b[36mcollection time: 7.820894832999329s | start time: 17128.658576021\u001b[0m\n",
      "stopping page load...\n",
      "articles collected... \u001b[36mcollection time: 7.3187636000002385s | start time: 17129.223677781\u001b[0m\n",
      "stopping page load...\n",
      "articles collected... \u001b[36mcollection time: 8.134194049998769s | start time: 17128.749684283\u001b[0m\n",
      "stopping page load...\n",
      "articles collected... \u001b[36mcollection time: 8.481751262999751s | start time: 17129.21786399\u001b[0m\n",
      "stopping page load...\n",
      "articles collected... \u001b[36mcollection time: 9.650832858002104s | start time: 17129.236528231\u001b[0m\n",
      "stopping page load...\n",
      "parse end time =  17146.105997587\n",
      "run time (parsing article) =  9.626683968999714\n",
      "parse end time =  17146.262715726\n",
      "run time (parsing article) =  16.434998518998327\n",
      "parse end time =  17146.265412447\n",
      "run time (parsing article) =  14.033265321999352\n",
      "parse end time =  17149.149417914\n",
      "run time (parsing article) =  17.13121237299856\n",
      "parse end time =  17149.254308584\n",
      "run time (parsing article) =  16.481549136999092\n",
      "parse end time =  17149.608493037\n",
      "run time (parsing article) =  12.723856488002639\n",
      "parse end time =  17149.694121814\n",
      "run time (parsing article) =  11.994090586998936\n",
      "parse end time =  17150.415124839\n",
      "run time (parsing article) =  11.526582901002257\n",
      "parse end time =  17150.474282076\n",
      "run time (parsing article) =  13.93219788100032\n",
      "\n",
      "Scraped 10 pages.\n",
      "run time =  25.734513938998134\n"
     ]
    }
   ],
   "source": [
    "# TODO: https://medium.com/@sakthiveltvt.thangaraj/introduction-to-nest-asyncio-for-python-developers-afd7bed44768\n",
    "# TODO: https://www.bing.com/search?pglt=161&q=asynchio+nesting\n",
    "\n",
    "import time\n",
    "import asyncio\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# articles = []\n",
    "# global job_ads\n",
    "job_ads = {}\n",
    "# job_ids = []\n",
    "\n",
    "\n",
    "def parse_article(el, job_ad_id):\n",
    "    job_ads[job_ad_id][(el.get_attribute(\"data-automation\")).replace('-','_')] = el.text \n",
    "\n",
    "def parse_articles(article):\n",
    "    start_time_outer = time.perf_counter()\n",
    "    try:\n",
    "        print(f'parsing articles')\n",
    "        job_id = article.get_attribute(\"data-job-id\")\n",
    "        print(f'jid: {c[2]}{job_id}{c[0]}')\n",
    "        job_ads[job_id], start_time_inner, elements = {}, time.perf_counter(), article.find_elements(By.CSS_SELECTOR, '[data-automation]')\n",
    "        print(f'{c[1]}find data elements time:{time.perf_counter() - start_time_inner}s | startime:{start_time_inner}{c[0]}')\n",
    "        # with ThreadPoolExecutor(max_workers=64) as executor:\n",
    "        #     result = executor.map(parse_article, elements, [job_id for i in elements])\n",
    "        # executor.shutdown(wait=True) # Shutdown the executor\n",
    "        for el in elements: #data-automation data-testid\n",
    "            job_ads[job_id][(el.get_attribute(\"data-automation\")).replace('-','_')] = el.text\n",
    "    except Exception as e:\n",
    "        print('Error parse_articles(): ', e)\n",
    "    finally:\n",
    "        print(f'{c[3]}parse article time = {time.perf_counter() - start_time_outer}, startime: {start_time_outer}{c[0]}')\n",
    "\n",
    "# Function to scrape one page using Selenium\n",
    "def scrape_with_selenium(url):\n",
    "    try:\n",
    "        # options = Options()\n",
    "        options = webdriver.ChromeOptions()\n",
    "        options.page_load_strategy = 'none'\n",
    "        driver = webdriver.Chrome(options=options)\n",
    "        driver.get(url)\n",
    "        print(f\"Scraped {url}\")\n",
    "        start_time = time.perf_counter()\n",
    "        articles = WebDriverWait(driver, 22).until(\n",
    "            EC.presence_of_all_elements_located((By.TAG_NAME, 'article'))\n",
    "        )\n",
    "        print(f'articles collected... {c[1]}collection time: {time.perf_counter() - start_time}s | start time: {start_time}{c[0]}')\n",
    "        start_time = time.perf_counter()\n",
    "        (print('stopping page load...'), driver.execute_script(\"window.stop();\"))\n",
    "        try:\n",
    "            # with ThreadPoolExecutor(max_workers=64) as executor:\n",
    "            #     print('EXECUTORðŸª“ðŸª“ðŸª“')\n",
    "            #     result = executor.map(parse_articles, articles) #articles[:5])\n",
    "            # executor.shutdown(wait=True) # Shutdown the executor\n",
    "            jobs = driver.execute_script(\"\"\"\n",
    "                let articles = Array.from(document.getElementsByTagName('article'))\n",
    "                let jobs = {}\n",
    "                articles.map(article => {\n",
    "                \tlet jobId = article.dataset.jobId\n",
    "                \tjobs[jobId] = {}\n",
    "                \tlet elements = Array.from(article.querySelectorAll('[data-automation]'))\n",
    "                \treturn elements.map(el => {\n",
    "                \t\tlet dataKey = el.dataset.automation\n",
    "                \t\tjobs[jobId][dataKey] = el.innerText\n",
    "                \t})\n",
    "                })\n",
    "                return jobs\n",
    "            \"\"\")\n",
    "            job_ads.update(jobs)\n",
    "        except Exception as e:\n",
    "            print('Error:', e)\n",
    "        finally:\n",
    "            driver.quit()\n",
    "        print('parse end time = ', time.perf_counter())\n",
    "        print('run time (parsing article) = ', time.perf_counter() - start_time)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print('Error: scrape_with_selenium()', e)\n",
    "    finally:\n",
    "        # driver.quit()\n",
    "        return articles\n",
    "\n",
    "# Async wrapper to run the blocking Selenium code\n",
    "async def async_scrape(url, executor):\n",
    "    try:\n",
    "        loop = asyncio.get_running_loop()\n",
    "        return await loop.run_in_executor(executor, scrape_with_selenium, url)\n",
    "    except Exception as e:\n",
    "        print('Error:', e)\n",
    "    \n",
    "async def populate_results(url, executor):\n",
    "    try:\n",
    "        article = await async_scrape(url, executor)\n",
    "        # articles.append(article)\n",
    "    except Exception as e:\n",
    "        print('Error:', e)\n",
    "\n",
    "# Main async function to scrape multiple URLs\n",
    "async def main(urls):\n",
    "    try:\n",
    "        results = []\n",
    "        with ThreadPoolExecutor(max_workers=128) as executor:\n",
    "            tasks = [populate_results(url, executor) for url in urls]\n",
    "            results = await asyncio.gather(*tasks)\n",
    "        print(f\"\\nScraped {len(results)} pages.\")\n",
    "    except Exception as e:\n",
    "        print('Error:', e)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        start_time = time.perf_counter()\n",
    "        urls = [\n",
    "        f'https://www.seek.com.au/Data-Engineer-jobs/in-Melbourne-VIC-3000?page={x+1}' for x in range(10)\n",
    "        ]\n",
    "        await main(urls)\n",
    "        print('run time = ', time.perf_counter() - start_time)\n",
    "    except Exception as e:\n",
    "        print('Error:', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "579cbe86-100d-4e51-9d8e-c7e0be25d688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for article in articles[0]:\n",
    "#     parse_articles(article, 'xyz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d2872441-d1e7-445b-93f1-fd0bde85bbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# job_ads = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9ab65877-ad4e-4b86-9a22-08df55622f82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "187"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(job_ads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8bf29ccb-ca59-4cfc-b0ef-7ba1b3e0ef93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'85163127': {'job_list_view_job_link': '',\n",
       "  'job_list_item_link_overlay': '',\n",
       "  'company_logo_container': '',\n",
       "  'company_logo': '',\n",
       "  'jobTitle': 'AI ENGINEER',\n",
       "  'jobCompany': 'Opus Recruitment Solutions',\n",
       "  'jobCardLocation': 'Melbourne VIC',\n",
       "  'jobLocation': 'Melbourne VIC',\n",
       "  'jobSalary': 'AUD 900 - 1150 per day',\n",
       "  'jobShortDescription': 'AI ENGINEER | 6 MONTH CONTRACT |MELBOURNE | HYBRID WORKING',\n",
       "  'jobSubClassification': '',\n",
       "  'jobClassification': '',\n",
       "  'signed_out_save_job': ''}}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "d = job_ads\n",
    "dict(itertools.islice(d.items(), 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385939f8-3810-46ea-9c19-1856061e5236",
   "metadata": {},
   "source": [
    "<h3 style=\"color: #F34222;\">VERSION 3</h3><p><h4 style=\"color: #F3FF22;\">using a queue</h4></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8636a52-eae1-46ee-80f1-66a0adfc387e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.bing.com/search?q=asyncio+queue+python+epxlained&cvid=d0b1708a716e47ba94a1cc3cc9f1a517&gs_lcrp=EgRlZGdlKgYIABBFGDkyBggAEEUYOTIGCAEQABhAMgYIAhAAGEAyBggDEAAYQDIGCAQQABhAMgYIBRAAGEAyBggGEAAYQDIGCAcQABhAMgYICBAAGEDSAQkxMDIwNGowajmoAgiwAgE&FORM=ANAB01&PC=U531\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599ac0a0-ca39-4732-89ab-33dabd348398",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfa7b7a-9659-4325-a011-a0c8a206f8f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69de1d1-427f-4c7e-b819-864f15fe0bf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b305b8-841e-4c11-bb3e-57498f4bc093",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4412811-14f1-4d9f-be08-bd41517c6a69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cbe183-bf1b-4399-840b-dcf6d3b2a195",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8cce063-9a58-412b-ab63-a2005408da90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import time\n",
    "# from operator import itemgetter\n",
    "\n",
    "# import asyncio\n",
    "# # import selenium\n",
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.support.ui import WebDriverWait\n",
    "# from selenium.webdriver.support import expected_conditions as EC\n",
    "# from selenium.webdriver.common.by import By\n",
    "# from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "\n",
    "# from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0eeef6e-4059-40a4-ab1a-7ce06032247a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Search():\n",
    "#     def __init__(self, role, loc, page_depth, page_type): #split non common attributes into distinct classes (search vs ad details)\n",
    "#         self.role = role\n",
    "#         self.loc = loc\n",
    "#         self.page_depth = page_depth\n",
    "#         self.page_type = page_type\n",
    "        \n",
    "#         self.urls = []\n",
    "#         self.drivers = []\n",
    "#         self.job_search_results = {}\n",
    "#         self.db_dir = f\"{os.getcwd()}/db/seek_app.sqlite\"\n",
    "#         self.ads_in_db = None\n",
    "#         # self.get_ads_in_db()\n",
    "#         self.ads_not_in_db = None\n",
    "#         self.SQL_command = None\n",
    "#         self.SQL_command_history = []\n",
    "        \n",
    "\n",
    "        \n",
    "#     def __str__(self):\n",
    "#         return f\"driver:{self.driver}\\nRole:{self.role}\\nLocation:{self.loc}\\nurls:{self.urls}\\nresults:{self.results}\\n\\n{self.job_ads}\\n{self.ads_in_db}\"\n",
    "\n",
    "    \n",
    "#     def generate_urls(self):\n",
    "#         postcode = {\n",
    "#             'Melbourne': {'state': 'VIC', 'postcode': 3000}\n",
    "#             , 'Sydney': {'state': 'NSW', 'postcode': 2000}\n",
    "#         }\n",
    "#         search_location_string = '-'.join(str(x) for x in itemgetter('state', 'postcode')(postcode.get(self.loc)))\n",
    "#         url = f'https://www.seek.com.au/{self.role.replace(\" \", \"-\")}-jobs/in-{self.loc}-{search_location_string}' # concat only if user sepcifies a location\n",
    "#         urls = [f'{url}?page={x+1}' for x in range(self.page_depth)]\n",
    "#         self.urls = urls\n",
    "        \n",
    "    \n",
    "#     def get_selenium_driver(self):\n",
    "#         print('get_selenium_driver()')\n",
    "#         CHROMEDRIVER_PATH=f'{os.getcwd()}/chromedriver-win64/chromedriver'\n",
    "#         try:\n",
    "#             options = webdriver.ChromeOptions()\n",
    "#             service = ChromeService()\n",
    "#             driver = webdriver.Chrome(service=service, options=options)\n",
    "#         except Exception as e:\n",
    "#             print('other error', e)\n",
    "#         return driver\n",
    "\n",
    "\n",
    "#     # Async wrapper to run the blocking Selenium code\n",
    "#     async def async_scrape(self, url, executor):\n",
    "#         loop = asyncio.get_running_loop()\n",
    "#         return await loop.run_in_executor(executor, self.scrape_with_selenium, url)\n",
    "        \n",
    "#     def scrape_with_selenium(self, url):\n",
    "#         try:\n",
    "#             driver = self.get_selenium_driver()\n",
    "#             self.drivers.append(driver)\n",
    "#             driver.get(url)\n",
    "#             article = driver.find_elements(By.TAG_NAME, \"article\")\n",
    "#             with ThreadPoolExecutor(max_workers=30) as executor:\n",
    "#                 executor.map(self.populate_results, article)\n",
    "#         except Exception as e:\n",
    "#             print('other error', e)\n",
    "#         finally:\n",
    "#             driver.quit()\n",
    "\n",
    "#     def populate_results(self, article):\n",
    "#         try:\n",
    "#             job_ad_id = article.get_attribute(\"data-job-id\")\n",
    "#             self.job_search_results[job_ad_id] = {}\n",
    "#             elements = article.find_elements(By.CSS_SELECTOR, '[data-automation]')\n",
    "#             for el in elements:\n",
    "#                 self.job_search_results[job_ad_id][el.get_attribute(\"data-automation\").replace('-','_')] = el.text\n",
    "#         except Exception as e:\n",
    "#             print('other error', e)\n",
    "\n",
    "#     # Main async function to scrape multiple URLs\n",
    "#     async def main(self, urls):\n",
    "#         results = []\n",
    "#         with ThreadPoolExecutor(max_workers=3000) as executor:\n",
    "#             tasks = [self.async_scrape(url, executor) for url in urls]\n",
    "#             results = await asyncio.gather(*tasks)\n",
    "#         print(f\"\\nScraped {len(results)} pages.\")\n",
    "\n",
    "#     def run_main(self):\n",
    "#         # Example usage\n",
    "#         if __name__ == \"__main__\":\n",
    "#             self.generate_urls()\n",
    "#             print(self.urls)\n",
    "            \n",
    "#             try:\n",
    "#                 # Works in notebooks or async environments\n",
    "#                 loop = asyncio.get_running_loop()\n",
    "#             except RuntimeError:\n",
    "#                 # No loop is running â€” safe to use asyncio.run()\n",
    "#                 print('exception')\n",
    "#                 asyncio.run(self.main(self.urls), debug=True)\n",
    "#             else:\n",
    "#                 # Loop is running â€” safe way to run async code inside it\n",
    "#                 print('using nest method')\n",
    "#                 import nest_asyncio\n",
    "#                 nest_asyncio.apply()\n",
    "#                 loop.create_task(self.main(self.urls))\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd3e79d8-5c4b-4e54-95af-2a98066cdc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# s = Search('Data Engineer', 'Melbourne', 10, 'search')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cbc064b-39d0-4236-9abd-9e8a601f1b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# s.run_main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14def999-7842-4a76-ab7f-4edd55fe7d61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3819e4e-369e-4dd6-96df-c54412a350f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @decorator_timer\n",
    "# def run_search(obj):\n",
    "#     obj.run_main()\n",
    "# # run_search(s)\n",
    "\n",
    "# result, exec_time = run_search(s)\n",
    "# print(exec_time) # prints after 0.07347989082336426 seconds\n",
    "# print(result) # takes ~3 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "49804bb5-ba69-433c-964b-8dda9710e5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decorator_timer(some_function):\n",
    "    from time import time\n",
    "\n",
    "    def wrapper(*args, **kwargs):\n",
    "        t1 = time()\n",
    "        result = some_function(*args, **kwargs)\n",
    "        end = time()-t1\n",
    "        return result, end\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272cb707-21ec-4e34-a7b7-5b1edc12f3ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1c8625-e3d6-4fe0-8b68-6347ee08397d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9532fcb5-b7a9-4052-a182-3a2e1178c051",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "23dc39a4-c3da-4880-8f15-03793ed3fe5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[URL('https://www.seek.com.au/Data-Engineer-jobs/in-Melbourne-VIC-3000?page=1'), URL('https://www.seek.com.au/Data-Engineer-jobs/in-Melbourne-VIC-3000?page=2'), URL('https://www.seek.com.au/Data-Engineer-jobs/in-Melbourne-VIC-3000?page=3'), URL('https://www.seek.com.au/Data-Engineer-jobs/in-Melbourne-VIC-3000?page=4'), URL('https://www.seek.com.au/Data-Engineer-jobs/in-Melbourne-VIC-3000?page=5'), URL('https://www.seek.com.au/Data-Engineer-jobs/in-Melbourne-VIC-3000?page=6'), URL('https://www.seek.com.au/Data-Engineer-jobs/in-Melbourne-VIC-3000?page=7'), URL('https://www.seek.com.au/Data-Engineer-jobs/in-Melbourne-VIC-3000?page=8'), URL('https://www.seek.com.au/Data-Engineer-jobs/in-Melbourne-VIC-3000?page=9'), URL('https://www.seek.com.au/Data-Engineer-jobs/in-Melbourne-VIC-3000?page=10')]\n",
      "Fetched https://www.seek.com.au/Data-Engineer-jobs/in-Melbourne-VIC-3000?page=1 with status 403\n",
      "\n",
      "Fetched 1 pages.\n",
      "run time =  0.05415487289428711\n"
     ]
    }
   ],
   "source": [
    "# import asyncio\n",
    "# import aiohttp\n",
    "# import time\n",
    "# import yarl\n",
    "\n",
    "# urls = [\n",
    "#     yarl.URL(f'https://www.seek.com.au/Data-Engineer-jobs/in-Melbourne-VIC-3000?page={x+1}', encoded=True) for x in range(10)\n",
    "# ]\n",
    "# print(urls)\n",
    "# # urls = [\n",
    "# #     f'https://www.seek.com.au/Data-Engineer-jobs/in-Melbourne-VIC-3000?page={x+1}' for x in range(10)\n",
    "# # ]\n",
    "# urls = ['https://www.seek.com.au/Data-Engineer-jobs/in-Melbourne-VIC-3000?page=1']\n",
    "\n",
    "# async def fetch(session, url):\n",
    "#     headers = {\n",
    "#         \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "#                       \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "#                       \"Chrome/113.0.0.0 Safari/537.36\",\n",
    "#         \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "#         \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "#         \"Accept-Encoding\": \"gzip, deflate\",\n",
    "#         \"Connection\": \"keep-alive\",\n",
    "#         \"Upgrade-Insecure-Requests\": \"1\",\n",
    "#     }\n",
    "#     async with session.get(url, headers=headers) as response:\n",
    "#         text = await response.text()\n",
    "#         print(f\"Fetched {url} with status {response.status}\")\n",
    "#         return text\n",
    "\n",
    "\n",
    "# async def fetch_all(urls):\n",
    "#     async with aiohttp.ClientSession() as session:\n",
    "#         tasks = [fetch(session, url) for url in urls]\n",
    "#         return await asyncio.gather(*tasks)\n",
    "\n",
    "\n",
    "# # Run this safely in a notebook or script\n",
    "# async def main():\n",
    "#     start_time = time.time()\n",
    "#     results = await fetch_all(urls)\n",
    "#     # print(results)\n",
    "#     print(f\"\\nFetched {len(results)} pages.\")\n",
    "#     print('run time = ', time.time() - start_time)\n",
    "\n",
    "\n",
    "# # Check if there's an existing event loop (Jupyter/IPython)\n",
    "# if __name__ == \"__main__\":\n",
    "#     try:\n",
    "#         # Works in notebooks or async environments\n",
    "#         loop = asyncio.get_running_loop()\n",
    "#     except RuntimeError:\n",
    "#         # No loop is running â€” safe to use asyncio.run()\n",
    "#         asyncio.run(main())\n",
    "#     else:\n",
    "#         # Loop is running â€” safe way to run async code inside it\n",
    "#         import nest_asyncio\n",
    "#         nest_asyncio.apply()\n",
    "#         loop.create_task(main())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
