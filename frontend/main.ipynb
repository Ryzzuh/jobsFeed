{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "898707d2-f80f-4afe-b214-c920e9246256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped https://www.seek.com.au/Data-Engineer-jobs/in-Melbourne-VIC-3000?page=10\n",
      "Scraped https://www.seek.com.au/Data-Engineer-jobs/in-Melbourne-VIC-3000?page=9\n",
      "Scraped https://www.seek.com.au/Data-Engineer-jobs/in-Melbourne-VIC-3000?page=3\n",
      "Scraped https://www.seek.com.au/Data-Engineer-jobs/in-Melbourne-VIC-3000?page=8\n",
      "Scraped https://www.seek.com.au/Data-Engineer-jobs/in-Melbourne-VIC-3000?page=5\n",
      "Scraped https://www.seek.com.au/Data-Engineer-jobs/in-Melbourne-VIC-3000?page=4\n",
      "Scraped https://www.seek.com.au/Data-Engineer-jobs/in-Melbourne-VIC-3000?page=1\n",
      "Scraped https://www.seek.com.au/Data-Engineer-jobs/in-Melbourne-VIC-3000?page=6\n",
      "Scraped https://www.seek.com.au/Data-Engineer-jobs/in-Melbourne-VIC-3000?page=2\n",
      "Scraped https://www.seek.com.au/Data-Engineer-jobs/in-Melbourne-VIC-3000?page=7\n",
      "articles collected... \u001b[36mcollection time: 1.0073391690020799s | start time: 17174.4679624\u001b[0m\n",
      "stopping page load...\n",
      "articles collected... \u001b[36mcollection time: 1.113316417002352s | start time: 17174.389026355\u001b[0m\n",
      "stopping page load...\n",
      "articles collected... \u001b[36mcollection time: 2.0092897600006836s | start time: 17174.457657254\u001b[0m\n",
      "stopping page load...\n",
      "articles collected... \u001b[36mcollection time: 2.4012309740028286s | start time: 17174.372982102\u001b[0m\n",
      "stopping page load...\n",
      "articles collected... \u001b[36mcollection time: 2.540609013998619s | start time: 17174.349864483\u001b[0m\n",
      "stopping page load...\n",
      "articles collected... \u001b[36mcollection time: 2.4930421960016247s | start time: 17174.564970351\u001b[0m\n",
      "stopping page load...\n",
      "parse end time =  17177.540701253\n",
      "run time (parsing article) =  2.057758285998716\n",
      "parse end time =  17177.588311665\n",
      "run time (parsing article) =  0.5302270880019933\n",
      "parse end time =  17177.602728595\n",
      "run time (parsing article) =  2.127185408000514\n",
      "articles collected... \u001b[36mcollection time: 3.281478707998758s | start time: 17174.343370062\u001b[0m\n",
      "stopping page load...\n",
      "parse end time =  17177.727122841\n",
      "run time (parsing article) =  1.2599015000014333\n",
      "articles collected... \u001b[36mcollection time: 5.7686817059984605s | start time: 17174.47540768\u001b[0m\n",
      "stopping page load...\n",
      "articles collected... \u001b[36mcollection time: 5.9246936140007165s | start time: 17174.325082337\u001b[0m\n",
      "stopping page load...\n",
      "articles collected... \u001b[36mcollection time: 6.314459674998943s | start time: 17174.367078709\u001b[0m\n",
      "stopping page load...\n",
      "parse end time =  17187.374859377\n",
      "run time (parsing article) =  10.485086397999112\n",
      "parse end time =  17189.010903755\n",
      "run time (parsing article) =  11.386581111997657\n",
      "parse end time =  17189.077100029\n",
      "run time (parsing article) =  8.827416864998668\n",
      "parse end time =  17190.071206719\n",
      "run time (parsing article) =  13.296858807996614\n",
      "parse end time =  17190.121581337\n",
      "run time (parsing article) =  9.87694219200057\n",
      "parse end time =  17190.268229964\n",
      "run time (parsing article) =  9.586208557997452\n",
      "\n",
      "Scraped 10 pages.\n",
      "run time =  20.424846425998112\n"
     ]
    }
   ],
   "source": [
    "# TODO: https://medium.com/@sakthiveltvt.thangaraj/introduction-to-nest-asyncio-for-python-developers-afd7bed44768\n",
    "# TODO: https://www.bing.com/search?pglt=161&q=asynchio+nesting\n",
    "\n",
    "import time\n",
    "import asyncio\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# ANSI colors\n",
    "c = (\n",
    "    \"\\033[0m\",   # End of color\n",
    "    \"\\033[36m\",  # Cyan\n",
    "    \"\\033[91m\",  # Red\n",
    "    \"\\033[35m\",  # Magenta\n",
    ")\n",
    "\n",
    "# articles = []\n",
    "# global job_ads\n",
    "job_ads = {}\n",
    "# job_ids = []\n",
    "\n",
    "\n",
    "def parse_article(el, job_ad_id):\n",
    "    job_ads[job_ad_id][(el.get_attribute(\"data-automation\")).replace('-','_')] = el.text \n",
    "\n",
    "def parse_articles(article):\n",
    "    start_time_outer = time.perf_counter()\n",
    "    try:\n",
    "        print(f'parsing articles')\n",
    "        job_id = article.get_attribute(\"data-job-id\")\n",
    "        print(f'jid: {c[2]}{job_id}{c[0]}')\n",
    "        job_ads[job_id], start_time_inner, elements = {}, time.perf_counter(), article.find_elements(By.CSS_SELECTOR, '[data-automation]')\n",
    "        print(f'{c[1]}find data elements time:{time.perf_counter() - start_time_inner}s | startime:{start_time_inner}{c[0]}')\n",
    "        # with ThreadPoolExecutor(max_workers=64) as executor:\n",
    "        #     result = executor.map(parse_article, elements, [job_id for i in elements])\n",
    "        # executor.shutdown(wait=True) # Shutdown the executor\n",
    "        for el in elements: #data-automation data-testid\n",
    "            job_ads[job_id][(el.get_attribute(\"data-automation\")).replace('-','_')] = el.text\n",
    "    except Exception as e:\n",
    "        print('Error parse_articles(): ', e)\n",
    "    finally:\n",
    "        print(f'{c[3]}parse article time = {time.perf_counter() - start_time_outer}, startime: {start_time_outer}{c[0]}')\n",
    "\n",
    "# Function to scrape one page using Selenium\n",
    "def scrape_with_selenium(url):\n",
    "    try:\n",
    "        # options = Options()\n",
    "        options = webdriver.ChromeOptions()\n",
    "        options.page_load_strategy = 'none'\n",
    "        driver = webdriver.Chrome(options=options)\n",
    "        driver.get(url)\n",
    "        print(f\"Scraped {url}\")\n",
    "        start_time = time.perf_counter()\n",
    "        articles = WebDriverWait(driver, 22).until(\n",
    "            EC.presence_of_all_elements_located((By.TAG_NAME, 'article'))\n",
    "        )\n",
    "        print(f'articles collected... {c[1]}collection time: {time.perf_counter() - start_time}s | start time: {start_time}{c[0]}')\n",
    "        start_time = time.perf_counter()\n",
    "        (print('stopping page load...'), driver.execute_script(\"window.stop();\"))\n",
    "        try:\n",
    "            # with ThreadPoolExecutor(max_workers=64) as executor:\n",
    "            #     print('EXECUTORðŸª“ðŸª“ðŸª“')\n",
    "            #     result = executor.map(parse_articles, articles) #articles[:5])\n",
    "            # executor.shutdown(wait=True) # Shutdown the executor\n",
    "            jobs = driver.execute_script(\"\"\"\n",
    "                let articles = Array.from(document.getElementsByTagName('article'))\n",
    "                let jobs = {}\n",
    "                articles.map(article => {\n",
    "                \tlet jobId = article.dataset.jobId\n",
    "                \tjobs[jobId] = {}\n",
    "                \tlet elements = Array.from(article.querySelectorAll('[data-automation]'))\n",
    "                \treturn elements.map(el => {\n",
    "                \t\tlet dataKey = el.dataset.automation\n",
    "                \t\tjobs[jobId][dataKey] = el.innerText\n",
    "                \t})\n",
    "                })\n",
    "                return jobs\n",
    "            \"\"\")\n",
    "            job_ads.update(jobs)\n",
    "        except Exception as e:\n",
    "            print('Error:', e)\n",
    "        finally:\n",
    "            driver.quit()\n",
    "        print('parse end time = ', time.perf_counter())\n",
    "        print('run time (parsing article) = ', time.perf_counter() - start_time)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print('Error: scrape_with_selenium()', e)\n",
    "    finally:\n",
    "        # driver.quit()\n",
    "        return articles\n",
    "\n",
    "# Async wrapper to run the blocking Selenium code\n",
    "async def async_scrape(url, executor):\n",
    "    try:\n",
    "        loop = asyncio.get_running_loop()\n",
    "        return await loop.run_in_executor(executor, scrape_with_selenium, url)\n",
    "    except Exception as e:\n",
    "        print('Error:', e)\n",
    "    \n",
    "async def populate_results(url, executor):\n",
    "    try:\n",
    "        article = await async_scrape(url, executor)\n",
    "        # articles.append(article)\n",
    "    except Exception as e:\n",
    "        print('Error:', e)\n",
    "\n",
    "# Main async function to scrape multiple URLs\n",
    "async def main(urls):\n",
    "    try:\n",
    "        results = []\n",
    "        with ThreadPoolExecutor(max_workers=128) as executor:\n",
    "            tasks = [populate_results(url, executor) for url in urls]\n",
    "            results = await asyncio.gather(*tasks)\n",
    "        print(f\"\\nScraped {len(results)} pages.\")\n",
    "    except Exception as e:\n",
    "        print('Error:', e)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        start_time = time.perf_counter()\n",
    "        urls = [\n",
    "        f'https://www.seek.com.au/Data-Engineer-jobs/in-Melbourne-VIC-3000?page={x+1}' for x in range(10)\n",
    "        ]\n",
    "        await main(urls)\n",
    "        print('run time = ', time.perf_counter() - start_time)\n",
    "    except Exception as e:\n",
    "        print('Error:', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "127c0811-d16e-4a45-a2ff-d294b5bd12de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_ads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ebc1ac-b035-4537-8ec0-69ffb5c2e4a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
